# Groq Model Configuration

## Current Setup

The app now uses **OpenAI-compatible models on Groq** instead of Llama:

### Text Chat - Groq Fallback Chain:
1. **Primary:** `openai/gpt-oss-120b` (120B parameters)
2. **Fallback:** `openai/gpt-oss-20b` (20B parameters)

If gpt-oss-120b exceeds rate limit â†’ automatically tries gpt-oss-20b

## Full AI Routing Chain

```
Text Message Sent
    â†“
ğŸ“¡ Try Groq gpt-oss-120b (streaming)
    â†“ (if fails)
ğŸ“¡ Try Groq gpt-oss-20b (streaming)
    â†“ (if fails)
ğŸ“¡ Try OpenRouter MythoMax (non-streaming)
    â†“ (if fails)
ğŸ“¡ Try Gemini 2.0 Flash Lite (non-streaming)
    â†“ (if all fail)
âŒ Show error message
```

## Console Output Examples

### Success with 120b:
```
ğŸ¤– Starting AI request routing...
ğŸ“¡ Attempting Groq API...
ğŸ“¡ Trying Groq with gpt-oss-120b...
âœ… Groq succeeded!
```

### Fallback to 20b:
```
ğŸ¤– Starting AI request routing...
ğŸ“¡ Attempting Groq API...
ğŸ“¡ Trying Groq with gpt-oss-120b...
âŒ gpt-oss-120b failed: Rate limit exceeded
ğŸ“¡ Fallback to gpt-oss-20b...
âœ… Groq succeeded!
```

### Both Groq models fail:
```
ğŸ¤– Starting AI request routing...
ğŸ“¡ Attempting Groq API...
ğŸ“¡ Trying Groq with gpt-oss-120b...
âŒ gpt-oss-120b failed: Rate limit exceeded
ğŸ“¡ Fallback to gpt-oss-20b...
âŒ gpt-oss-20b failed: Rate limit exceeded
âŒ Groq failed (both models): [error]
ğŸ“¡ Attempting OpenRouter API (fallback 1)...
âœ… OpenRouter succeeded!
```

## Model Details

### openai/gpt-oss-120b
- Size: 120 billion parameters
- Speed: Very fast on Groq
- Quality: High
- Free tier: Limited requests

### openai/gpt-oss-20b
- Size: 20 billion parameters  
- Speed: Very fast on Groq
- Quality: Good
- Free tier: More generous limits

## Why This Setup?

1. **Best quality first** (120b) for most users
2. **Automatic downgrade** (20b) when rate limited
3. **Multiple fallbacks** to other providers
4. **Streaming support** on both Groq models
